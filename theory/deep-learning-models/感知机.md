**概念**：感知机是二类分类的线性分类模型，输入实例的特征向量，输出实例的类别
<img width="715" alt="截屏2025-03-30 19 56 24" src="https://github.com/user-attachments/assets/c49cd8f6-fea0-4533-9808-9b7cea860bf4" />

感知机使用特征向量来表示二元分类器，把矩阵上输入x（实数值向量）映射到输出值y上（一个二元的值）

$$
f(x)=\begin{cases}
+1,if(w·x+b>0)\\
-1,else
\end{cases}
$$

输入信号被送往神经元时会被乘以固定的权重，权重越大，对应的权重信号的重要性就越高
<img width="732" alt="截屏2025-04-10 10 34 18" src="https://github.com/user-attachments/assets/da4913d5-1a45-4004-8cad-2be3d4d88239" />

%%w是实数的表示权重的向量，w·x是点积；b是偏置，一个不依赖于任何输入值的参数%%
<img width="623" alt="截屏2025-03-30 20 13 55" src="https://github.com/user-attachments/assets/14e46ba5-9023-4471-823f-a88d4a767e9a" />

- 对于错误分类的数据点 $(x_i,y_i）$ 总有 $-y_i·（w·x_i +b）>0$ 
- 错误分类点到直线 $L$ 的距离为 $\frac{1}{||w||}$  $|w·x_i+b|$ 
- 假设直线 $L$ 的误分类点集合为$m$,那么所有误分类点到直线 $L$ 的总距离为

$$
\frac{1}{||w||} \sum\limits_i^m y_i(w·x_i+b)
$$

不考虑 $\frac{1}{||w||}$ ，感知机的损失函数为 $K(w,b)=-\sum\limits_i^m y_i·(w·x_i+b)$ 

`感知机的优化算法采用的是随机梯度下降算法`

**激活函数**：

sign()

//当函数的输入值大于0时，函数的值为+1；当函数的输入值小于0时，函数的值为-1

当我们输入一个二维的特征向量（ $x_1$ , $x_2$ )

 $$
 y=\begin{cases}
-1 （w_1 ·x_1 +w_2 ·x_2 +b）\le0\\
1  (w_1 ·x_1 +w_2 ·x_2 +b）>0
\end{cases}
 $$
 
# 创建二位数据并用线性回归模型拟合出直线的代码 

```python
import numpy as np
from sklearn.datasets import make_classification
from sklearn import linear_model
import matplotlib.pyplot as plt


td_data = make_classification(n_samples=20, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=24)
td_data = list(td_data)
td_data[1] = np.array([1 if i == 0 else -1 for i in td_data[1]])
td_data = tuple(td_data)

fig, ax = plt.subplots()
scatter = ax.scatter(td_data[0][:, 0], td_data[0][:, 1], c=td_data[1], alpha=0.6, cmap="cool")
legend_1 = ax.legend(*scatter.legend_elements(), title="Classes", loc="upper left")
ax.add_artist(legend_1)
ax.set_xlabel("Feature_1")
ax.set_ylabel("Feature_2")
  
reg = linear_model.LinearRegression()
reg.fit(td_data[0][:, 0].reshape(-1, 1), td_data[0][:, 1].reshape(-1, 1))
print(f"the intercept is {reg.intercept_[0]} and the coefficient is {reg.coef_[0][0]}")
formula = f"f(x)={round(reg.coef_[0][0], 2)}*x1-x2{round(reg.intercept_[0], 2)}"

create_x_axis = np.linspace(min(td_data[0][:, 0]), max(td_data[0][:, 0]), 100).reshape(-1, 1)
predicted_value = reg.predict(create_x_axis)
  
ax.plot(create_x_axis, predicted_value, c="gold", alpha=0.8, label=formula)
handles, labels = ax.get_legend_handles_labels()
legend_2 = ax.legend(handles, labels, loc="lower right")

plt.show()
```

## 1. 导入库
```python
import numpy as np
from sklearn.datasets import make_classification
from sklearn import linear_model
import matplotlib.pyplot as plt
import random
```
- `numpy`: 用于数值计算
- `make_classification`: 生成分类数据集
- `linear_model`: 线性模型(这里使用线性回归)
- `matplotlib.pyplot`: 绘图
- `random`: 随机数生成(虽然代码中未使用)

## 2. 生成分类数据
```python
td_data = make_classification(n_samples=20, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=24)
```
- 生成20个样本，2个特征
- 2个信息特征，0个冗余特征
- 每个类1个簇
- random_state=24确保可重复性
- 返回格式: (data, target)

## 3. 转换标签
```python
td_data = list(td_data)
td_data[1] = np.array([1 if i == 0 else -1 for i in td_data[1]])
td_data = tuple(td_data)
```
- 将原始元组转为列表以便修改
- 将标签0转为1，标签1转为-1
- 再转换回元组格式

## 4. 创建图形
```python
fig, ax = plt.subplots()
```
- 创建图形和坐标轴对象

## 5. 绘制散点图
```python
scatter = ax.scatter(td_data[0][:, 0], td_data[0][:, 1], c=td_data[1], alpha=0.6, cmap="cool")
```
- x轴: 第一个特征
- y轴: 第二个特征
- 颜色由标签决定(1或-1)
- 透明度0.6
- 使用"cool"颜色映射

## 6. 添加图例
```python
legend_1 = ax.legend(*scatter.legend_elements(), title="Classes", loc="upper left")
ax.add_artist(legend_1)
```
- 为散点图添加分类图例
- 标题"Classes"
- 位置在左上角

## 7. 设置坐标轴标签
```python
ax.set_xlabel("Feature_1")
ax.set_ylabel("Feature_2")
```

## 8. 创建和训练线性回归模型
```python
reg = linear_model.LinearRegression()
reg.fit(td_data[0][:, 0].reshape(-1, 1), td_data[0][:, 1].reshape(-1, 1))
```
- 创建线性回归模型
- 用第一个特征预测第二个特征
- reshape确保数据是二维的

## 9. 打印模型参数
```python
print(f"the intercept is {reg.intercept_[0]} and the coefficient is {reg.coef_[0][0]}")
```
- 输出截距和系数

## 10. 创建回归线公式字符串
```python
formula = f"f(x)={round(reg.coef_[0][0], 2)}*x1-x2{round(reg.intercept_[0], 2)}"
```
- 格式化为f(x)=a*x1-x2b的形式

## 11. 生成预测值
```python
create_x_axis = np.linspace(min(td_data[0][:, 0]), max(td_data[0][:, 0]), 100).reshape(-1, 1)
predicted_value = reg.predict(create_x_axis)
```
- 在x轴范围内生成100个点
- 用模型预测对应的y值

## 12. 绘制回归线
```python
ax.plot(create_x_axis, predicted_value, c="gold", alpha=0.8, label=formula)
```
- 金色线条
- 透明度0.8
- 标签为回归方程

## 13. 添加回归线图例
```python
handles, labels = ax.get_legend_handles_labels()
legend_2 = ax.legend(handles, labels, loc="lower right")
```
- 获取所有图例句柄和标签
- 在右下角添加回归线图例

## 14. 显示图形
```python
plt.show()
```

### 代码功能总结
这段代码:
1. 生成一个二分类数据集
2. 将标签转换为1/-1
3. 绘制数据点的散点图
4. 用线性回归拟合特征间的关系
5. 绘制回归线并显示方程
6. 最终展示分类数据和回归线的可视化结果

注意: 这里线性回归是在两个特征之间做的，而不是用于分类任务，可能只是为了展示特征间的关系。
